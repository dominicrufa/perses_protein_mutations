{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here, I extend the existing `ala_to_ser_and_cys_to_ser_analysis.ipynb` and test the speed of simulation timesteps in solvent, as well as simulation speed in vacuum with a `STRONG` atom map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymbar import timeseries\n",
    "from pymbar import MBAR\n",
    "from perses.analysis import utils\n",
    "import os\n",
    "\n",
    "from openmmtools.multistate import MultiStateReporter, MultiStateSamplerAnalyzer\n",
    "import networkx as nx\n",
    "from itertools import combinations \n",
    "import time\n",
    "import pickle\n",
    "from perses.dispersed.utils import minimize\n",
    "# from ala_to_ser_and_cys_to_ser import query_constraints, print_hybrid_atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## it looks like ALA --> SER (and reverse) works; let's look at the topologies and systems to see the transformation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_constraints(htf):\n",
    "    \"\"\"\n",
    "    this function will pull constraints from the hybrid system and print the names of the atoms with constraints and the constraint parameters...\n",
    "    \"\"\"\n",
    "    hybrid_sys = htf._hybrid_system\n",
    "    num_constraints = hybrid_sys.getNumConstraints()\n",
    "    constraints = []\n",
    "    for i in range(num_constraints):\n",
    "        constraints.append(hybrid_sys.getConstraintParameters(i))\n",
    "    \n",
    "    hybr_to_old = htf._hybrid_to_old_map\n",
    "    hybr_to_new = htf._hybrid_to_new_map\n",
    "    old_top, new_top = htf._topology_proposal._old_topology,  htf._topology_proposal._new_topology\n",
    "    old_top_dict = {atom.index: (atom.residue.name, atom.name) for atom in old_top.atoms()}\n",
    "    new_top_dict = {atom.index: (atom.residue.name, atom.name) for atom in new_top.atoms()}\n",
    "    \n",
    "    counter = 0\n",
    "    for (i, j, distance) in constraints:\n",
    "        print(f\"hybrid indices: {i}, {j}\")\n",
    "        new, old = False, False\n",
    "        in_loop_counter = 0\n",
    "        try: \n",
    "            old_idx_i,old_idx_j = hybr_to_old[i], hybr_to_old[j]\n",
    "            old_atom_i, old_atom_j = old_top_dict[old_idx_i], old_top_dict[old_idx_j]\n",
    "            print('\\t', old_atom_i, old_atom_j, distance)\n",
    "            old = True\n",
    "            in_loop_counter +=1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            new_idx_i,new_idx_j = hybr_to_new[i], hybr_to_new[j]\n",
    "            new_atom_i, new_atom_j = new_top_dict[new_idx_i], new_top_dict[new_idx_j]\n",
    "            print('\\t', new_atom_i, new_atom_j, distance)\n",
    "            new = True\n",
    "            in_loop_counter +=1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        if old and new:\n",
    "            print(f\"\\tthis is core\")\n",
    "            in_loop_counter = 1\n",
    "        elif old and not new:\n",
    "            print(f\"\\tthis is old\")\n",
    "        elif not old and new:\n",
    "            print(f\"\\tthis is new\")\n",
    "        else:\n",
    "            print(f\"\\tthere is a problem\")\n",
    "        \n",
    "        counter += in_loop_counter\n",
    "    \n",
    "    assert counter == len(constraints), f\"uh oh\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hybrid_atoms(htf):\n",
    "    hybrid_sys = htf._hybrid_system\n",
    "    hybr_to_old = htf._hybrid_to_old_map\n",
    "    hybr_to_new = htf._hybrid_to_new_map\n",
    "    old_top, new_top = htf._topology_proposal._old_topology,  htf._topology_proposal._new_topology\n",
    "    old_top_dict = {atom.index: (atom.residue.name, atom.name) for atom in old_top.atoms()}\n",
    "    new_top_dict = {atom.index: (atom.residue.name, atom.name) for atom in new_top.atoms()}\n",
    "    for particle_idx in range(hybrid_sys.getNumParticles()):\n",
    "        print(f\"hybrid_index: {particle_idx}\")\n",
    "        print(f\"hybrid mass: {hybrid_sys.getParticleMass(particle_idx)}\")\n",
    "        new, old = False, False\n",
    "        try:\n",
    "            new_atom = new_top_dict[hybr_to_new[particle_idx]]\n",
    "            print(f\"\\tnew atom map: {new_atom}\")\n",
    "            new = True\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try: \n",
    "            old_atom = old_top_dict[hybr_to_old[particle_idx]]\n",
    "            print(f\"\\told atom map: {old_atom}\")\n",
    "            old = True\n",
    "        except: \n",
    "            pass\n",
    "        \n",
    "        if old and new:\n",
    "            print(f\"\\tcore atom\")\n",
    "        elif old and not new:\n",
    "            print(f\"\\told atom\")\n",
    "        elif not old and new:\n",
    "            print(f\"\\tnew atom\")\n",
    "        else:\n",
    "            print(f\"\\tthis atom is undefined\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ALA_SER.solvent.default_map.pkl', 'rb') as f:\n",
    "    ALA_SER = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('CYS_SER.solvent.default_map.pkl', 'rb') as f:\n",
    "    CYS_SER = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1548"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALA_SER._hybrid_system.getNumConstraints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simtk import openmm\n",
    "from openmmtools import integrators\n",
    "import simtk.openmm as openmm\n",
    "from openmmtools.constants import kB\n",
    "import simtk.unit as unit\n",
    "temperature = 300 * unit.kelvin\n",
    "kT = kB * temperature\n",
    "beta = 1.0/kT\n",
    "from openmmtools.states import ThermodynamicState, SamplerState, CompoundThermodynamicState\n",
    "from openmmtools import mcmc, utils\n",
    "import openmmtools.cache as cache\n",
    "from perses.dispersed.utils import configure_platform\n",
    "import openmmtools.integrators as integrators\n",
    "from openmmtools import mcmc, utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_langevin_integrator(system, positions, constraint_tol):\n",
    "    \"\"\"\n",
    "    create lambda alchemical states, thermodynamic states, sampler states, integrator, and return context, thermostate, sampler_state, integrator\n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    fast_thermodynamic_state = ThermodynamicState(system, temperature = temperature)\n",
    "    \n",
    "    fast_sampler_state = SamplerState(positions = positions, box_vectors = system.getDefaultPeriodicBoxVectors())\\\n",
    "    \n",
    "    integrator_1 = integrators.LangevinIntegrator(temperature = temperature,\n",
    "                                                     timestep = 4.0* unit.femtoseconds,\n",
    "                                                     splitting = 'V R O R V',\n",
    "                                                     measure_shadow_work = False,\n",
    "                                                     measure_heat = False,\n",
    "                                                     constraint_tolerance = constraint_tol,\n",
    "                                                     collision_rate = 5.0 / unit.picoseconds)\n",
    "\n",
    "    \n",
    "    \n",
    "    #print(integrator_1.getConstraintTolerance())\n",
    "    \n",
    "    fast_context, fast_integrator = cache.global_context_cache.get_context(fast_thermodynamic_state, integrator_1)\n",
    "    \n",
    "    \n",
    "    fast_sampler_state.apply_to_context(fast_context)\n",
    "    \n",
    "    #minimize\n",
    "    minimize(fast_thermodynamic_state, fast_sampler_state)\n",
    "    \n",
    "    \n",
    "    return fast_context, fast_thermodynamic_state, fast_sampler_state, fast_integrator\n",
    "\n",
    "def time_lan_integrator(integrator, num_steps = 10):\n",
    "    import time\n",
    "    _time = []\n",
    "    integrator.step(1)\n",
    "    #move.apply(thermostate, sstate)\n",
    "    for i in range(num_steps):\n",
    "        start = time.time()\n",
    "        integrator.step(1)\n",
    "        #move.apply(thermostate, sstate)\n",
    "        end = time.time() - start\n",
    "        _time.append(end)\n",
    "    return np.array(_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_pdb(topology, positions, output_pdb = 'test_new.pdb'):\n",
    "    \"\"\"\n",
    "    create a pdb of the geometry proposal (only new system)\n",
    "    \"\"\"\n",
    "    import mdtraj as md\n",
    "    _positions =  np.array(positions.value_in_unit(unit.nanometer))\n",
    "    print(_positions)\n",
    "    traj = md.Trajectory(_positions, md.Topology.from_openmm(topology))\n",
    "    traj.save(output_pdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:sMC_utils:using global context cache\n",
      "DEBUG:sMC_utils:using global context cache\n"
     ]
    }
   ],
   "source": [
    "ALA_SER_context, ALA_SER_thermodynamic_state, ALA_SER_sampler_state, ALA_SER_integrator = create_langevin_integrator(ALA_SER._hybrid_system, ALA_SER._hybrid_positions, 1e-6)\n",
    "CYS_SER_context, CYS_SER_thermodynamic_state, CYS_SER_sampler_state, CYS_SER_integrator = create_langevin_integrator(CYS_SER._hybrid_system, CYS_SER._hybrid_positions, 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.00000100e-01  1.00000000e-01 -1.30000000e-07]\n",
      " [ 2.00000100e-01  2.09000000e-01  1.00000000e-08]\n",
      " [ 1.48626400e-01  2.45384900e-01  8.89824000e-02]\n",
      " ...\n",
      " [-7.09523750e-01 -1.79800000e-01  1.50599985e-01]\n",
      " [ 1.92876250e-01  1.30790000e+00  1.57479999e+00]\n",
      " [ 7.69876250e-01 -4.36700000e-01  1.90479998e+00]]\n",
      "[[ 2.00000100e-01  1.00000000e-01 -1.30000000e-07]\n",
      " [ 2.00000100e-01  2.09000000e-01  1.00000000e-08]\n",
      " [ 1.48626400e-01  2.45384900e-01  8.89824000e-02]\n",
      " ...\n",
      " [-7.09523750e-01 -1.79800000e-01  1.50599985e-01]\n",
      " [ 1.92876250e-01  1.30790000e+00  1.57479999e+00]\n",
      " [ 7.69876250e-01 -4.36700000e-01  1.90479998e+00]]\n"
     ]
    }
   ],
   "source": [
    "create_new_pdb(ALA_SER._topology_proposal._new_topology, ALA_SER._new_positions, output_pdb = 'ala_ser_solvent.pdb')\n",
    "create_new_pdb(CYS_SER._topology_proposal._new_topology, CYS_SER._new_positions, output_pdb = 'cys_ser_solvent.pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALA_SER_times, CYS_SER_times = time_lan_integrator(ALA_SER_integrator, num_steps = 1000), time_lan_integrator(CYS_SER_integrator, num_steps = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004523008346557617 0.0016671330598278938\n",
      "0.004359082460403442 0.0015907414927481677\n"
     ]
    }
   ],
   "source": [
    "print(np.average(ALA_SER_times), np.std(ALA_SER_times))\n",
    "print(np.average(CYS_SER_times), np.std(CYS_SER_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wow, so now the speed difference goes away in CPU?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALA_SER_state = ALA_SER_context.getState(getEnergy=True, \n",
    "                                         getForces=True, \n",
    "                                         getPositions=True,\n",
    "                                         getVelocities=True)\n",
    "CYS_SER_state = CYS_SER_context.getState(getEnergy=True, \n",
    "                                         getForces=True, \n",
    "                                         getPositions=True,\n",
    "                                         getVelocities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['ALA_SER.solvent.default_map.state.xml', 'CYS_SER.solvent.default_map.state.xml']\n",
    "for filename, _state in zip(filenames, [ALA_SER_state, CYS_SER_state]):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(openmm.openmm.XmlSerializer.serialize(_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just for good measure, what happens if we make deepcopies of the htf.hybrid systems, remove the constraints, and run again?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what happens when we use a strong mapping in vacuum?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:sMC_utils:using global context cache\n",
      "DEBUG:sMC_utils:using global context cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000464937686920166 3.876953237497666e-05\n",
      "0.0004548287391662598 3.5257700215192345e-05\n"
     ]
    }
   ],
   "source": [
    "with open('ALA_SER.vacuum.strong_map.pkl', 'rb') as f:\n",
    "    ALA_SER = pickle.load(f)\n",
    "with open('CYS_SER.vacuum.strong_map.pkl', 'rb') as f:\n",
    "    CYS_SER = pickle.load(f)\n",
    "    \n",
    "ALA_SER_context, ALA_SER_thermodynamic_state, ALA_SER_sampler_state, ALA_SER_integrator = create_langevin_integrator(ALA_SER._hybrid_system, ALA_SER._hybrid_positions, 1e-6)\n",
    "CYS_SER_context, CYS_SER_thermodynamic_state, CYS_SER_sampler_state, CYS_SER_integrator = create_langevin_integrator(CYS_SER._hybrid_system, CYS_SER._hybrid_positions, 1e-6)\n",
    "\n",
    "ALA_SER_times, CYS_SER_times = time_lan_integrator(ALA_SER_integrator, num_steps = 1000), time_lan_integrator(CYS_SER_integrator, num_steps = 1000)\n",
    "\n",
    "print(np.average(ALA_SER_times), np.std(ALA_SER_times))\n",
    "print(np.average(CYS_SER_times), np.std(CYS_SER_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALA_SER_state = ALA_SER_context.getState(getEnergy=True, \n",
    "                                         getForces=True, \n",
    "                                         getPositions=True,\n",
    "                                         getVelocities=True)\n",
    "CYS_SER_state = CYS_SER_context.getState(getEnergy=True, \n",
    "                                         getForces=True, \n",
    "                                         getPositions=True,\n",
    "                                         getVelocities=True)\n",
    "\n",
    "filenames = ['ALA_SER.vacuum.strong_map.state.xml', 'CYS_SER.vacuum.strong_map.state.xml']\n",
    "for filename, _state in zip(filenames, [ALA_SER_state, CYS_SER_state]):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(openmm.openmm.XmlSerializer.serialize(_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_system = copy.deepcopy(CYS_SER._hybrid_system)\n",
    "mod_htf = copy.deepcopy(CYS_SER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_constraints(mod_htf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's just delete the last constraint\n",
    "num_const = mod_system.getNumConstraints()\n",
    "mod_system.removeConstraint(num_const - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CYS_SER_mod_context, CYS_SER_mod_thermodynamic_state, CYS_SER_mod_sampler_state, CYS_SER_mod_integrator = create_langevin_integrator(mod_system, CYS_SER._hybrid_positions, 1e-6)\n",
    "CYS_SER_mod_times = time_lan_integrator(CYS_SER_mod_integrator, num_steps = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(CYS_SER_times), np.std(CYS_SER_times))\n",
    "print(np.average(CYS_SER_mod_times), np.std(CYS_SER_mod_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, if we delete the new constraint between the hybrid OG/SG and the unique new HG, then we recover speed...why is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
